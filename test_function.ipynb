{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcJK3kXl--c3"
      },
      "source": [
        "# Computer Vision Project\n",
        "\n",
        "Sajeel Nadeem Alam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rVDkKZEEKIp"
      },
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3D_ONirhSvw"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl3ZyAAVEKI1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxIKS3ukEKI2"
      },
      "source": [
        "### Define Local Path\n",
        "\n",
        "In the next cell you should assign to the variable `GOOGLE_DRIVE_PATH_AFTER_MYDRIVE` the relative path of this folder in your Google Drive.\n",
        "\n",
        "**IMPORTANT:** you have to make sure that **all the files required to test your functions are loaded using this variable** (as was the case for all lab tutorials). In other words, do not use in the notebook any absolute paths. This will ensure that the markers can run your functions. Also, **do not use** the magic command `%cd` to change directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28VgE7dMEKI2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/ComputerVision/Coursework/CW_Folder_PG'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "747B4GKgvHsv"
      },
      "source": [
        "### Load packages\n",
        "\n",
        "In the next cell you should load all the packages required to test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlNicHnRkrcp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, rc\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "from joblib import dump, load\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage.feature import hog\n",
        "from skimage.transform import resize\n",
        "\n",
        "!pip install mtcnn\n",
        "!pip install lz4\n",
        "\n",
        "from mtcnn import MTCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ag6odkk1Ki"
      },
      "source": [
        "### Load models\n",
        "\n",
        "In the next cell you should load your best performing model (this might consist of more than one file). Avoid to load it within `MaskDetection` to avoid having to reload it each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCpF4Aa6lhnO"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # this part of the code is adpated from Lab 8 [2]\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 3)\n",
        "model = model.to(device)\n",
        "\n",
        "model_path = os.path.join(GOOGLE_DRIVE_PATH, 'Models','bestCNNClassifier.pth')\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc83ETI1a3o9"
      },
      "source": [
        "# Test MaskDetection\n",
        "\n",
        "This section should allow to test the `MaskDetection` function. First, add cells with the code needed to load the necessary subroutines to make `MaskDetection` work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbAKlkNBU7CQ"
      },
      "outputs": [],
      "source": [
        "rc('animation', html='jshtml', embed_limit=50)\n",
        "\n",
        "def MaskDetectionAnimation(path):\n",
        "    cap = cv2.VideoCapture(path)                            # opens the video which is at the provided path [1]\n",
        "    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))     # gets the total number of frames in the video [1]\n",
        "    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))     # gets the width of each frame [1]\n",
        "    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))   # gets the height of each frame [1]\n",
        "    print(f\"Frame count: {frameCount}, Width: {frameWidth}, Height: {frameHeight}\")\n",
        "\n",
        "    mtcnn = MTCNN()      # creates a MTCNN object to detect faces [2]\n",
        "\n",
        "    model.eval()                            # sets the CNN model to evaluation mode\n",
        "    dataMeans = [0.485, 0.456, 0.406]       # ImageNet mean values to normalise each channel in the image\n",
        "    dataStds = [0.229, 0.224, 0.225]        # ImageNet standard deviation values to normalise each channel in the image\n",
        "    transformTest = transforms.Compose([    # defines transformation to be applied to the image [2]\n",
        "        transforms.Resize(256),             # resizes shorter side of the image to 256 pixels\n",
        "        transforms.CenterCrop(224),         # crops a 224*224 pixel square from the centre of the image\n",
        "        transforms.ToTensor(),              # converts the image to a tensor\n",
        "        transforms.Normalize(dataMeans, dataStds)  # uses the defined means and standard deviations to normalise the image\n",
        "    ])\n",
        "\n",
        "    video = []            # initalises an empty list to store processed video frames\n",
        "    annotations = []      # initalises an empty list to store the bounding boxes and predictions\n",
        "    for i in range(0, frameCount, 5):         # loops over the video and selects every frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)    # sets the current frame position in the video to the ith position\n",
        "        ret, frame = cap.read()                # reads the frame at the current position; ret indicates whether it was read\n",
        "        if not ret:                            # successfully or not. It it was not read successfully then the loop is exited\n",
        "            break\n",
        "\n",
        "        rgbFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    # converts frame from BGR to RGB as required by MTCNN\n",
        "        faces = mtcnn.detect_faces(rgbFrame)                 # extracts a list of faces and their properties from the frame\n",
        "        bbox = None                            # initialises bounding box to None\n",
        "        pred = None                            # initalises prediction to None\n",
        "\n",
        "        if faces:                           # checks for faces in the list and only proceeds if there are any\n",
        "            x, y, w, h = faces[0]['box']    # extracts the coordinates for the top left corner and the width and height of the\n",
        "            bbox = (x, y, w, h)             # bounding box of the first face (my video only has one face) and saves them as a tuple\n",
        "            faceCrop = rgbFrame[y:y+h, x:x+w]  # crops the face from the frame\n",
        "\n",
        "            # transforms the face crop using the defined transformations, adds a batch dimension and moves the tensor to the GPU\n",
        "            faceTensor = transformTest(Image.fromarray(faceCrop)).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():                       # makes sure that gradients are not computed while getting predictions\n",
        "                out = model(faceTensor)                 # uses the model to get the prediction for the face tensor\n",
        "                pred = torch.argmax(out, dim=1).item()  # gets the class with the highest score and converts it to a scalar\n",
        "\n",
        "            video.append(rgbFrame)                  # appends the frame to video list\n",
        "            annotations.append((bbox, int(pred)))   # appends the bounding box along with the prediction to the annotations list\n",
        "\n",
        "    cap.release()     # releases the video capture object\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))  # creates a figure and axes for plotting the video [1]\n",
        "\n",
        "    def frame(i):           # defines the call back function which is called to display each frame [1]\n",
        "        ax.clear()          # clears the  previous frame's axes so that the current frame can be displayed correctly [1]\n",
        "        ax.axis('off')      # turns the axes off, getting rid of ticks and labels for clearer visualisation [1]\n",
        "        img = video[i]                # retrieves the ith frame from the list of frames (video) and stores in img\n",
        "        bbox, pred = annotations[i]   # retrieves and stores the corresponding bounding boxes and prediction\n",
        "        ax.imshow(img)                # displays the current frame\n",
        "\n",
        "        predDict = {0: 'No mask', 1: 'Mask', 2: 'Mask Incorrect'}   # sets corresponding class names\n",
        "        if bbox:                      # checks whether the frame has a bounding box\n",
        "            x, y, w, h = bbox         # extracts the coordinates of the bounding box and uses them to add a patch to the axes which creates the bounding box\n",
        "            ax.add_patch(patches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=2)) # [2]\n",
        "            ax.text(x, y - 10, f\"{predDict[pred]}\", color='red', fontsize=12, weight='bold')  # add the predicted class on top of the bounding box\n",
        "\n",
        "    anim = animation.FuncAnimation(fig, frame, frames=len(video))  # creates an animation object. Fig is where the frames will be displayed\n",
        "    plt.close()                                                         # and frame is the call back function called at each frame. Plot is closed\n",
        "    return anim                                                   # after animation object is created. Animation object is then returned [1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHVBUe82pLz6"
      },
      "source": [
        "Then, make a call to the `MaskDetection` function to see what results it produces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k928S7snTOX"
      },
      "outputs": [],
      "source": [
        "video_path = os.path.join(GOOGLE_DRIVE_PATH, 'Personal_Video/testVideo.mp4')\n",
        "cap_info = cv2.VideoCapture(video_path)\n",
        "if not cap_info.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_path} to get FPS.\")\n",
        "    original_fps = 30 # Default to 30 if cannot read\n",
        "else:\n",
        "    original_fps = cap_info.get(cv2.CAP_PROP_FPS)\n",
        "    cap_info.release()\n",
        "print(f\"Original video FPS: {original_fps}\")\n",
        "\n",
        "path_to_test = os.path.join(GOOGLE_DRIVE_PATH, 'Personal_Video/testVideo.mp4')\n",
        "anim = MaskDetectionAnimation(path_to_test)\n",
        "\n",
        "if anim:\n",
        "    output_animation_path_sampled = os.path.join(GOOGLE_DRIVE_PATH, 'Personal_Video/maskDetectionOutput_SampledVideo.mp4')\n",
        "    try:\n",
        "        # Calculate the effective FPS for the sampled video.\n",
        "        # If original video is 30 FPS and you take every 10th frame,\n",
        "        # you effectively have 3 frames per second of original content.\n",
        "        effective_fps = original_fps / 5\n",
        "        print(f\"Calculated effective FPS for sampled video: {effective_fps}\")\n",
        "\n",
        "        # Save with high DPI, no borders, and the adjusted effective FPS.\n",
        "        anim.save(output_animation_path_sampled, fps=effective_fps, dpi=200,\n",
        "                          savefig_kwargs={'bbox_inches': 'tight', 'pad_inches': 0})\n",
        "        print(f\"Sampled animation successfully saved to: {output_animation_path_sampled}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving the sampled animation: {e}\")\n",
        "else:\n",
        "    print(\"Sampled animation object was not created. Check video path or function errors.\")\n",
        "\n",
        "anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aieelNogp_6N"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] G. Tarroni, Lab Tutorial 04, Computer Vision - IN3060/INM460, School of Science & Technology, Department of Computer Science, City, University of London, London, U.K., 2025\n",
        "\n",
        "[2] G. Tarroni, Lab Tutorial 08, Computer Vision - IN3060/INM460, School of Science & Technology, Department of Computer Science, City, University of London, London, U.K., 2025"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
